---
title: "Forecasting of USD/CAD exchange rate using methods for time series analysis"
author: "Arina Sitnikova"
date: "December 2019"
output:
  pdf_document: default
  html_document: default
header-includes: \onehalfspacing
geometry: margin = 1in
fontsize: 11pt
---
<style>
body {
text-align: justify}
</style>

##### Executive summary

Nowadays national economies are extremely integrated. Globalization inevitably leads us to day-to-day interaction among people, companies, and governments worldwide. The exchange rate is considered to be one of the key indicators of relative price in open economies. Knowing more than other agents makes it is possible to gain a lot of benefits and profit from international trade. For this reason, forecasting of exchange rates is relevant and in demand. As such, many researchers tried to determine the most suitable model – from general financial models to new methods of deep machine learning – which would have the highest possible prediction ability. In most recent studies, authors used neural networks and similar techniques; however, the results often turned out to be contradictory depending on the currency.

In this paper, we aimed to predict the exchange rate of the US dollar to the Canadian dollar using data from the year 1971 to 2019. We built two types of forecast-specific models: ARIMA and VAR. While ARIMA is an element of a class of models which explains a given series based on its past values – or lags, VAR is used for multiple time series (i.e. with more than one evolving variable), and each variable is a function of not only its past lags but also past lags of all other variables. For the VAR model, we decided to use CPI, interest rate and the unemployment rate as the additional variables based on the previous research. 

The performance of each model was compared against all other models. We used several accuracy metrics as the criterion (and never faced contradictions) but considered MSE to be the most illustrative one because of the assumptions and structure of the data (e.g. absence of outliers). In addition, there were several specifications of the VAR model: we tried different combinations of variables based on the correlation between the exchange rate and three other variables. We also examined Granger causality which helped us to determine whether one series was useful in forecasting another. It turned out that interest and unemployment rates were not nearly as important as CPI. 

As a result, despite the complexity of the VAR model, it was not as efficient in forecasting as the ARIMA model was. One of the possible reasons was found after we performed the variance decomposition which indicates the amount of information each variable contributes to the other variables. We found out that the exchange rate’ variance was mostly explained by its own shocks, not by other variables, and in that case, VAR estimation might not have been as helpful, unlike ARIMA. Besides, it is also should be noted that the parsimonious specification of ARIMA did not capture any trends and performed extremely poorly; for this reason, it was wise to estimate a model with a greater number of lags. Nevertheless, neither of the models reached a near-perfect accuracy because of a sudden shock which occurred at the beginning of 2019.  

##### Introduction and Literature Review

The objective of this paper is to predict the US dollar to Canadian dollar exchange rate. Apart from a univariate analysis where the future path will be predicted with the help of the previous values (lags) of the USD/CAD exchange rate, we will perform a multivariate analysis that usually shows better results due to the fact that the values of other variables are used in support of the initial forecast objective. Therefore, a researcher must choose appropriate variables that would make an actual impact on the investigated variable. We will also support our choice of the variables by inspecting Granger causality. Finally, we will examine the performance of the models at a 12-months forecast horizon using differing metrics (such primary accuracy measures as MSE, MAE, etc) and check impulse response functions to understand the direction of effects of other variables on the dependent variable (i.e. exchange rate). In the end, it will be possible to determine which model shows the best performance and whether its prediction is correct. 

The results of this research might be important since the exchange rate is one of the most important indicators of a country's level of the economy. A country's trading highly depends on exchange rates: for example, a lower-valued national currency results in more expensive imports and cheaper imports, which directly affects a country's level of output. However, not only governments watch and analyze exchange rates. Likewise, the exchange rate is important for firms that work with foreign customers or suppliers. Forecast of exchange rate will help agents to adjust for new circumstances and avoid potential losses. With a view of the future, this research can be developed further by applying more complex methods such as artificial intelligence techniques which are believed to have higher prediction ability than traditional econometric methods. 

There is a lot of scientific articles in which authors try a wide range of models, from traditional economic theory to deep learning methods, to get the most accurate prediction of future exchange rates. Some of the authors who have investigated this problem a lot are Cheung et al. who estimated eight models to predict various exchange rates (USD to the Canadian dollar, British pound, Swiss franc, etc.) and compared them against the random walk model. Four of those models were estimated in Cheung et al. (2005) and the other four were covered in Cheung et al. (2017). The models used include purchasing power parity, the sticky-price monetary model of Dornbusch and Frankel, interest rate parity, a sticky-price monetary model, etc. As a result, none of the models was specifically successful in the MSE criteria. The authors concluded that the exchange rates appeared difficult to predict using the models examined in the study. Another deep study of exchange rates was performed by Babu and Reddy (2015) - they tried to forecast exchange rates of the Indian Rupee against several other widespread currencies using ARIMA, Neural Network and Fuzzy Neuron model (which is also a part of deep learning). Out of three models, the fuzzy model deviates much from the actual for all the four currencies; other models' performance is more or less satisfactory. However, although neural nets are considered to be the most complex models and therefore their predictive ability is supposed to be the highest, the authors concluded that the ARIMA model had the best performance based on AIC/BIC criterion.  

One of the main differences between our research and previous literature is that I will perform traditional ARIMA and VAR models instead of brand new techniques such as deep machine learning - some of the researchers claim that older models might perform at least not worse than the modern ones. In addition, we will use the original set of explanatory variables in VAR analysis. The choice was also based on the literature which covered time series applications, including the VAR model. For example, Ito and Sato (2006) stated in their paper that the exchange rate is determined by, among others, the interest rate, inflation rate, and other macroeconomic variables that are subject to monetary and fiscal policies. As a result, they build a VAR model using a vector of two variables: the exchange rate and inflation rate. Similarly, M. Simionescu (2017) made the exchange rate forecasts using VAR and Bayesian VAR models for monthly US/Euro exchange rate and consumer price index. Lastly, J.Mida (2013) had a purpose of forecasting the USD/EUR exchange rate for the year of 2012 using short-term interest rate, inflation rate, unemployment rate and industrial production index as variables that directly interact with this exchange rate.

Based on the above, in the second part of this paper, we intend to predict the USD/CAD exchange rate for 2019 using the following explanatory variables: CPI as a measure of domestic inflation; interest rate and the unemployment rate that proved to be the primary macroeconomic indicators affecting the exchange rate. The reason for choosing the interest rate lies in the fact that the changes in interest rates affect significantly capital flows. When it grows, there is an inflow of foreign capital, as an investment in a country is becoming more attractive. As a result, there is an increased demand for domestic currency, which will then be valued at a higher price against foreign currencies. The changes in the inflation rate must have the opposite effect. When the price level goes up, the PPP against other countries decreases, making the currency depreciate since the increased prices on products of a given country discourage foreign countries to import them. Speaking of the unemployment rate, it is one of the indicators of relative economic health. Low unemployment rates are usually associated with a good economic situation. That boosts consumer spending, investment and demand for the currency. As a result, the domestic currency is likely to be more valuable. 

The final data consists of four columns: USD to CAD exchange rate, Consumer Price Index, interest rate and the unemployment rate in Canada. The examined period is from January 1971 to October 2019 (exchange rate - to December 2019).

##### Univariate analysis

To begin with, the original monthly data of the USD/CAD exchange rate was converted to the time series format. The series started in January 1971 and ended in December 2019. In total, there are 588 observations.

```{r, message=FALSE, warning=FALSE, include=FALSE}
library(readxl)
library(lmtest)
library(tidyverse)
library(forecast)
library(gridExtra)
library(psych)
library(knitr)
library(kableExtra)
ex_rate <- read_excel("~/Desktop/exch_data_cad2.xlsx")
rate_ts <- ts(ex_rate, start = c(1971, 1), frequency = 12)
```

```{r, include=FALSE}
train = window(rate_ts, end = c(2018,12))
```

The series is plotted on the Figure 1 (can be found in the attachments).

```{r, include=FALSE}
autoplot(rate_ts) + ggtitle("Figure 1. Exchange Rate, USD to CAD") + ylab('Exchange Rate')
```

We can assume that there are several structural breaks. A structural break occurs when we see a sudden change in a time series, which might be the case for our series. Structural breaks can lead to huge forecasting errors and unreliability of the model in general. If structural breaks are present, we should take this into account to avoid errors and make our forecast more precise. So, we run an automatic procedure that detects possible structural breaks. The time when it was likely to occur is presented below:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(strucchange)
bp_ts <- breakpoints(rate_ts ~ 1)
#summary(bp_ts) 
break_point <- breakpoints(rate_ts ~ 1)
break_date <- breakdates(break_point)
f1 <- c(rep(0, 91), rep(1, 277 - 91), rep(0, length(train) - 277))
f2 <- c(rep(0, 277), rep(1, 405 - 277), rep(0, length(train) - 405))
f3 <- c(rep(0, 405), rep(1, 500 - 405), rep(0, length(train) - 500))
f4 <- c(rep(0, 500), rep(1, length(train) - 500))
matrr <- matrix(c(f1,f2,f3,f4), ncol = 4)
cat("Years with possible structural breaks:", round(breakdates(break_point), 0))
```

We will keep in mind these years. Also, we create a matrix which reflects the four structural breaks to consider them in the further analysis. 

Furthermore, to understand whether the series had a trend or seasonal pattern. If it does, it is necessary to take it into account while running ARIMA model as ignoring that can result in inconsistent estimation.
Thus, the series was decomposed; the decomposition can be viewed in the Figure 2 called "Decomposition of additive time series".

```{r, eval=FALSE, include=FALSE}
plot(decompose(rate_ts))
```

As seen, the series does not have a determined upward or downward trend; the only thing we can see there is uneven pattern and fluctuations. Apart from this, we can conclude that even if there is any seasonal component, it is totally negligible: the scale of the seasonality graph is too small (from -.004 to .002). To sum up, it is highly likely that we've got neither trend nor seasonal pattern.

After this, it is necessary to find out whether our original series is stationary or non-stationary. There are two plots of ACF and PACF presented in the Figure 3.

```{r, eval=FALSE, include=FALSE}
grid.arrange(ggAcf(rate_ts), ggPacf(rate_ts), nrow = 2)
```

For a stationary series, it is expected to see autocorrelations which would decay to zero at higher lags. Clearly, it is not the case here: autocorrelations only slightly decrease over lags. Therefore, we can assume nonstationarity. To make sure, we will perform two tests on stationarity: ADF test (which takes H0 as nonstationarity) and KPSS test (in which H0 stands for stationarity).

The result of the ADF test:
```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tseries)
adf.test(rate_ts, k = 10)
```

P-value is even greater than 0.1, then we can conclude that the null hypothesis of nonstationarity cannot be rejected at any adequate level of confidence.

The result of the KPSS test:
```{r, echo=FALSE}
kpss.test(rate_ts, null = "Level")
```

P-value is less than 0.01; therefore we can reject the null hypothesis of stationarity at the 1% level, which confirms the results of the ADF test. Thus, the original series is not stationary. To get rid of non-stationarity, the first difference was taken. The result can be seen in Figure 4. In addition, Figure 5 reflects ACF and PACF plots. 

```{r, include=FALSE}
diff_rate <- diff(rate_ts)
autoplot(diff_rate) + ggtitle("Figure 4. Change in Exchange Rate, USD to CAD") + ylab("Exchange rate difference")
```

```{r, include=FALSE}
grid.arrange(ggAcf(diff_rate), ggPacf(diff_rate), nrow = 2)
```

Now the data seems to be stationary. Getting back to seasonality, the absence of regular spikes at specific lags (for example, spikes at the 4th, 8th, 12th lag and so on means quarterly seasonality) supports the assumption that there is no significant seasonal components. 

Again, we take a look at the following tests to confirm that the differenced series is stationary:

```{r, echo=FALSE}
adf.test(diff_rate, k = 10)
kpss.test(diff_rate, null = "Level", lshort = TRUE) 
```

So, the ADF test shows that H0 is rejected (since p-value is smaller than 0.01), and the KPSS test states that H0 is accepted at any adequate level of confidence (since p-value is larger than 0.1), which means that now the series is stationary.

To find out how accurate our forecast will be, we'll forecast the last available year at first. There is no point in predicting more than 12-24 months as the forecast will be highly inaccurate. But first of all, it is necessary to estimate ARIMA model. One of the possible ways to choose the best value for p and q lags it to take a look at ACF and PACF graphs of the train data (Figure 6). 

```{r, include=FALSE}
grid.arrange(ggAcf(diff(train)), ggPacf(diff(train)), nrow = 2)
```

So, it can be assumed that the original data (not differenced) may be analyzed as in ARIMA(4,1,1). The number of p is determined by looking at the PACF graph; the ACF plot determines the number of q. The d was already proven to be equal to 1 as the first difference appeared to be stationary. However, the best way is to iterate through all the possible models and choose the best one according to AIC criteria. As Box-Jenkins approach recommends that the model be parsimonious, we set the maximum number of lags at five. Apart from this, we include the matrix consisted of four vectors which represent four structural breaks that we discussed above. The results of the Arima estimation are presented below:

```{r, echo=FALSE}
matr = matrix(0, nrow = 6, ncol = 6)
for (i in 0:5){
  for (j in 0:5) {
    model = Arima(train, order=c(i,1,j), include.constant = TRUE, xreg = matrr)
    matr[i+1,j+1] = model$aic
  }
}

matr <- data.frame(matr)
colnames(matr) <- c('q=0', 'q=1', 'q=2', 'q=3', 'q=4', 'q=5')
rownames(matr) <- c('p=0', 'p=1', 'p=2', 'p=3', 'p=4', 'p=5')
matr %>% kable() %>% kable_styling()
```

```{r, echo=FALSE}
sprintf("The minimum AIC is: %f", min(matr))
```

The minimum AIC is found at p = 2, q = 4. After the estimation, we can verify that all lags are significant at least at 5% level. However, neither the drift nor structural breaks vectors are significant, which means we can exclude them:
```{r, echo=FALSE}
coeftest(Arima(train, order = c(2,1,4), include.constant = TRUE, xreg = matrr))
```

```{r, include=FALSE}
fit2 <- Arima(train, order = c(2,1,4))
```

After this, we estimate the ARIMA(2,1,4) model. Before building a forecast, it is important to examine residuals:
```{r, echo=FALSE}
Box.test(residuals(fit2), lag = 20, type = 'Ljung')
jarque.bera.test(residuals(fit2))
```

First, the Box-Ljung autocorrelation test was performed. The null hypothesis is that there is no autocorrelation, and since p-value is greater than 0.05, it cannot be rejected. However, having conducted the Jarque Bera Test for normality, it turned out that residuals are not normal (H0 was rejected). Nevertheless, forecasting can still be done in that case. 

```{r, include=FALSE}
fore2 <- forecast(fit2, h = 12)
```

Finally, the forecast is illustrated in the Figue 7. For convenience, not the whole series is shown - only from the year of 2005.

```{r, message=FALSE, warning=FALSE, include=FALSE}
actual_data <- window(rate_ts, start = c(2018, 12), end = c(2019,12))
autoplot(fore2, xlim = c(2005,2020)) + autolayer(actual_data, series = "Actual Exchange Rate") + autolayer(fore2$mean, series = "Forecast") + ylab('Exchange Rate')
```

From what we can see, the forecast is not as accurate, but the errors are not critical - the predicted values are relatively close to the real ones. Moreover, the actual values are within 80% confidence interval.

There is also automatic auto.arima function which is useful for us since it can either support or contradict our choice of lags. The results for auto.arima are presented below:

```{r, echo=FALSE}
fit0 <- auto.arima(train, d = 1, seasonal = TRUE)
fit0
```

As seen, the model offered by computer differs from what we chose manually. It is more parsimonious though AIC is greater. However, the MA coefficient is still significant as we can say from the test of coefficients:

```{r, echo=FALSE}
coeftest(Arima(train, order = c(0,1,1)))
```

Then the autocorrelation and normality tests are performed: 

```{r, echo=FALSE}
Box.test(residuals(fit0), lag = 10, type = 'Ljung')
jarque.bera.test(residuals(fit0))
```

Again, no autocorrelation is detected, but the residuals are not normal. Therefore, we can still make a forecast. It is presented graphically in the Figure 8 (again, the graph reflects only 2005 and later for convenience).

```{r, include=FALSE}
fore0 <- forecast(fit0, h = 12)
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
autoplot(fore0, xlim = c(2005,2020)) + autolayer(actual_data, series = "Actual Exchange Rate") + autolayer(fore0$mean, series = "Forecast") + ylab('Exchange Rate')
```

On the contrary, this forecast does not look to be accurate at all (it does not reflect any trends and is constant during all 12 periods) and seems to be worse than the previous one. However, the real values are still within the confidence interval. To determine which forecast is better, it is worth taking a look at accuracy measures.

Accuracy measures for ARIMA(2,1,4):

```{r, echo=FALSE}
options(digits = 5)
acc01 <- data.frame(accuracy(fore0, actual_data))
acc01 %>% kable() %>% kable_styling()
```

Accuracy measures for ARIMA(0,1,1):

```{r, echo=FALSE}
options(digits = 5)
acc00 <- data.frame(accuracy(fore2, actual_data))
acc00 %>% kable() %>% kable_styling()
```

On the test set, errors are smaller in the first case. Therefore, it can be concluded that it is likely that ARIMA(2,1,4) performs better in case of the 12-month univariate forecast of the USD/CAD exchange rate.

Lastly, the table of forecast and actual values for this model (ARIMA(2, 1, 4):
```{r, echo=FALSE}
compare <- data.frame(cbind(fore2$mean, actual_data[-1]))
colnames(compare) <- c('Forecast', 'Real Data')
rownames(compare) <- c('Jan 2019', 'Feb 2019', 'Mar 2019', 'Apr 2019', 'May 2019', 'Jun 2019', 'Jul 2019', 'Aug 2019', 'Sep 2019', 'Oct 2019', 'Nov 2019', 'Dec 2019')
compare$differ <- compare[,2] - compare[,1]
colnames(compare) <- c('Forecast', 'Real Data', 'Difference')
compare %>% kable() %>% kable_styling()
```

##### Multivariate analysis

As discussed above, now we use monthly data of the CAD/USD exchange rate, CPI (base year is 2015), interest rate and unemployment rate, starting from January 1971 to October 2019 with 564 observations in total.

```{r, include=FALSE}
library(fpp)
library(vars)
library(readxl)
ts_data <- read_excel("~/Desktop/ts_data2.xls")
series <- ts(ts_data, start = c(1971, 1), frequency = 12)
actual_data2 <- window(series, start = c(2018, 10), end = c(2019,10))
```

The plot of the series is presented in the Figure 9.

```{r, include=FALSE}
autoplot(series) + ggtitle("Figure 9. Series of Four Macroeconomic Indicators") + ylab('Value') 
```

No tests needed to verify that this multivariate time series is not stationary: for example, CPI variable has a determined trend upwards. So, we can choose whether we will proceed with this series or take the first difference to make the series stationary. Regardless of the choice, it is still possible to make a forecast and analyze the series properly. We decided to stay with nonstationary series. Therefore, we should keep in mind that we have to take a rather big number of lags - this is necessary for all further actions, such as building IRF, estimating FEDV and so on. We decided to limit the maximum number of lags to 20 before proceeding to the automatic lag selection procedure.

The results of the lag selection:
```{r, echo=FALSE}
train2 = window(series, end = c(2018,10))
VARselect(train2, lag.max = 20, type = "const")$selection
```

Two criteria stated that two lags are at best, and the other two selected nine lags. Usually, it's up to a researcher to decide in this case. We will estimate the model with nine lags as we have to stick to the huge number of lags.

```{r, include=FALSE}
var1 <- VAR(train2, p = 9, type = "const")
#summary(var1)
```

To check the presence of autocorrelation in the VAR(9) model, we perform an asymptotic Portmanteau (a significant number of observations allows it) and BG tests that check the null hypothesis that there is no remaining residual autocorrelation at lags:

```{r, echo=FALSE}
serial.test(var1, lags.pt = 10, type = "PT.asymptotic") 
serial.test(var1, lags.pt = 10, type = "BG")  
```

Both tests show that autocorrelation is present (the null hypothesis is rejected), which makes it impossible to make an adequate forecast. Moreover, the residuals are not normal:

```{r, echo=FALSE}
normality.test(var1, multivariate.only = TRUE)$jb.mul
```

Since it is impossible to proceed with the forecast, we should change the model. In practice, it is usual to keep N small and include only variables that are correlated and therefore useful in forecasting each other. Then it can be helpful to get rid of one variable that does not have as much impact on the exchange rate. Let's take a look at the correlation matrix:

```{r, echo=FALSE}
correl <- data.frame(cor(series, y = NULL, use = "all.obs", method = c("pearson")))
correl %>% kable() %>% kable_styling()
```

From this table, we can see a more or less significant correlation between exchange rate and CPI, exchange rate and unemployment, but almost no correlation between exchange rate and interest rate. If we drop it, the performance might be better (at least, we may be able to make predictions), so we will try this option. 

With the new series consisted of exchange rate, CPI and unemployment rate, it was suggested using VAR(13) or VAR(2) model based on the VAR selection:
```{r, echo=FALSE}
data3 <- data.frame(train2)
data3 <- data3[-3]
series_3 <- ts(data3, start = c(1971, 1), frequency = 12)
VARselect(series_3, lag.max = 20, type = "const")$selection
var3 <- VAR(series_3, p = 13, type = "const")
```

Again, since we have nonstationary series, we choose and estimate the model with a bigger number of lags. The asymptotic PT and normality tests were performed as well:

```{r, echo=FALSE}
serial.test(var3, lags.pt = 20, type = "PT.asymptotic") 
normality.test(var3, multivariate.only = TRUE)
```

The Portmanteau test did not reject the null hypothesis of no autocorrelation this time. Although the residuals are not normal, now we can make a forecast anyway. 

Before this, it will be helpful to check Granger Causality. There is no point in keeping a variable if it does not help to improve a forecast of the initial variable (in our case, exchange rate). The results are presented below:

```{r, echo=FALSE}
causality(var3, "cpi")$Granger
causality(var3, "exchange")$Granger
causality(var3, "unemp")$Granger
```

It appears that at the 90% level, CPI measure Granger-causes both exchange and unemployment rate (null hypothesis about the opposite can be rejected). As well, at the 95% level, exchange rate Granger-causes CPI and unemployment rate. This means that at least CPI can improve the forecast of the exchange rate (and vice versa). Additionally, there is an instantaneous causality between CPI and exchange rate, CPI and unemployment rate, exchange rate and unemployment rate. Lastly, the unemployment rate does not Granger-cause exchange rate and CPI together, but it is impossible to detect individual impact here. 

To check the relationship between the unemployment rate and exchange rate, we decided to limit the series to these two variables only. Meanwhile, it is possible to estimate the VAR(6) model for unemployment and exchange rate since there is no autocorrelation (refer to VAR selection procedure and autocorrelation test below). What we want to know now is the relationship between exchange rate and unemployment rate:

```{r, echo=FALSE}
###VAR(6) w/ unemp and exch rate 
data2 <- data.frame(train2)
data2 <- data2[-c(2,3)]
series_2 <- ts(data2, start = c(1971, 1), frequency = 12)
VARselect(series_2, lag.max = 20, type = "const")$selection
var2 <- VAR(series_2, p = 6, type = "const")
serial.test(var2, lags.pt = 10, type = "PT.asymptotic") 
```

```{r, echo=FALSE}
causality(var2, 'exchange')$Granger
causality(var2, 'unemp')$Granger
```

So, it turned out that unemployment rate does not Granger-cause exchange rate (all of the coefficients on lagged values of unemployment rate are zero in the equation for exchange rate), which means it does not help to improve the forecast, however exchange rate does Granger-cause unemployment rate.

Since the unemployment rate variable does not seem to help to improve the forecast of exchange rate future values, we also decided to build the third model which consisted of the vector of two variables: CPI and exchange rate. In short, the model with 13 lags was estimated, no autocorrelation was found although residuals are not normal. Besides, it was confirmed that CPI Granger-causes the exchange rate. All of these findings can be confermed using the results presented below: 

```{r, echo=FALSE}
#VAR(13) for cpi and unemp
data4 <- data.frame(train2)
data4 <- data4[c('cpi', 'exchange')]
series_4 <- ts(data4, start = c(1971, 1), frequency = 12)
VARselect(series_4, lag.max = 20, type = "const")$selection 
var4 <- VAR(series_4, p = 13, type = "const")
serial.test(var4, lags.pt = 20, type = "PT.asymptotic") #no autocor
```

```{r, echo=FALSE}
causality(var4, 'exchange')$Granger
causality(var4, 'cpi')$Granger #Granger causes exchange again
```

To sum up, we decided to make a forecast for all three models that we mentioned and compare their performance:

1) VAR(13) with three variables: exchange rate, CPI, unemployment rate;

2) VAR(6) with two variables: exchange rate, unemployment rate;

3) VAR(13) with two variables: exchange rate, CPI.


```{r, message=FALSE, warning=FALSE, include=FALSE}
#FORECAST VAR(13) w/ three variables
varfore3 <- forecast(var3, h = 12)
plot3 <- autoplot(varfore3$forecast$exchange, xlim = c(2005,2020),  main = "Exchange Rate Forecast from VAR(13)") + autolayer(actual_data2[,1], series = "Actual Value") + autolayer(varfore3$forecast$exchange$mean, series = "Forecast") + ylab('Exchange Rate')
plot4 <- autoplot(varfore3$forecast$unemp, xlim = c(2000,2020),  main = "Unemployment Rate Forecast") + autolayer(actual_data2[,'unemp'], series = "Actual Value") + autolayer(varfore3$forecast$unemp$mean, series = "Forecast") + ylab('Unemployment Rate')
plot44 <- autoplot(varfore3$forecast$cpi, xlim = c(2000,2020),  main = "CPI Forecast") + autolayer(actual_data2[,'cpi'], series = "Actual Value") + autolayer(varfore3$forecast$unemp$mean, series = "Forecast") + ylab('CPI')
#grid.arrange(plot3, plot4, ncol = 1)
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
#Forecast VAR(6) w/ unemp and exchange
varfore <- forecast(var2, h = 12)
plot1 <- autoplot(varfore$forecast$exchange, xlim = c(2000,2020),  main = "Exchange Rate Forecast from VAR(6)") + autolayer(actual_data2[,1], series = "Actual Value") + autolayer(varfore$forecast$exchange$mean, series = "Forecast") + ylab('Exchange Rate')
#plot2 <- autoplot(varfore$forecast$unemp, xlim = c(2000,2018),  main = "Unemployment Rate Forecast from VAR(6)") + autolayer(actual_data2[,4], series = "Actual Value") + autolayer(varfore$forecast$unemp$mean, series = "Forecast") + ylab('Unemployment Rate')
#grid.arrange(plot1, plot2, ncol = 1)
```

```{r, message=FALSE, warning=FALSE, include=FALSE}
#Forecast VAR(13) w/ cpi and exchange
varfore4 <- forecast(var4, h = 12)
plot7 <- autoplot(varfore4$forecast$exchange, xlim = c(2000,2020),  main = "Exchange Rate Forecast from VAR(13)") + autolayer(actual_data2[,1], series = "Actual Value") + autolayer(varfore4$forecast$exchange$mean, series = "Forecast") + ylab('Exchange Rate')
#plot8 <- autoplot(varfore4$forecast$cpi, xlim = c(2000,2018),  main = "CPI Forecast from VAR(13)") + autolayer(actual_data2[,2], series = "Actual Value") + autolayer(varfore4$forecast$cpi$mean, series = "Forecast") + ylab(' CPI')
#grid.arrange(plot7, plot8, ncol = 1)
```

Accuracy measures of the exchange rate forecast for three multivariate models are presented below. We also added the results from the univariate forecast:

```{r, echo=FALSE}
acc1 <- data.frame(accuracy(varfore$forecast$exchange$mean, actual_data2[,1])[1,])  #unemp and exchange
acc3 <- data.frame(accuracy(varfore3$forecast$exchange$mean, actual_data2[,1])[1,]) #best three variables 
acc4 <- data.frame(accuracy(varfore4$forecast$exchange$mean, actual_data2[,1])[1,]) # cpi and exchange
acc2 <- data.frame(accuracy(fore2, actual_data)[2,])
acc2 <- acc2[-6,]
dfcomp <- cbind(acc3, acc1, acc4, acc2)
colnames(dfcomp) <- c("Multivariate1 (CPI+unemp+exchange)", "Multivariate2 (exchange+unemp)", "Multivariate3 (CPI+exchange)", "Univariate")
dfcomp %>% kable() %>% kable_styling()
```

As seen from the table, the first multivariate model (that has three variables - exchange rate, cpi and unemployment rate) has the largest error measures. It can be possibly explained by the fact that we saw that the unemployment rate does not Granger-cause the exchange rate (that's why the second model also performed relatively poorly. This was not the case with CPI and the exchange rate, that's why the third forecast performed much better and had the lowest error in each accuracy measure. However, we can point out that each of the multivariate models performed worse than the univariate one. This was unexpected: the variables chosen for the VAR model have a direct impact on the exchange rate; therefore, their present and past values were likely to help predict the future values of the exchange rate, but now we see quite the opposite. 

To make sure that the last statement was not wrong, there is a Figure 10 showing the best exchange rate forecast (i.e. VAR(13) with CPI and exchange rate).

The table of actual and forecast values is presented below:

```{r, message=FALSE, warning=FALSE, include=FALSE}
plot7
```

```{r, echo=FALSE}
comparew <- data.frame(cbind(varfore4$forecast$exchange$mean, actual_data2[-1,1]))
colnames(comparew) <- c('Forecast', 'Real Data')
rownames(comparew) <- c('Nov 2018', 'Dec 2018', 'Jan 2019', 'Feb 2019', 'Mar 2019', 'Apr 2019', 'May 2019', 'Jun 2019', 'Jul 2019', 'Aug 2019', 'Sep 2019', 'Oct 2019')
comparew$differ2 <- comparew[,2] - comparew[,1]
colnames(comparew) <- c('Forecast', 'Real Data', 'Difference')
comparew %>% kable() %>% kable_styling()
```

What we can actually see is that indeed ARIMA model managed to get more correct prediction in comparison to VAR. While VAR was trying to capture more complex dependencies, it was unable to capture a sudden shock which made the real exchange rate go up sharply (and instead the predictions made by VAR had a steady trend downwards). ARIMA simply relied on the previous lags and therefore its predictions tended to the average of the previous values which was the key to success in our case.

Moving to the structural analysis, the order of variables is vital here - the result will differ depending on the ordering. Therefore, we decided to reorder data so that CPI be ahead of the exchange rate as it is more powerful based on the previous tests. The unemployment rate was set in the last column as it does not have a substantial effect on either the exchange rate or CPI.

VAR is actually a reduced form model. Structural VAR reflects the implied structure among variables in the system. It is impossible to estimate SVAR directly, so we have to estimate a reduced-form VAR and then restore the parameters of the structural form. For example, we can compute $(B_0)^{-1}$ using Choleski decomposition:
```{r, eval=FALSE, include=FALSE}
#Omega_hat <- summary(var3)$covres
#chol(Omega_hat) %>% t()
#Psi(var4)[,,1]
```

```{r, echo=FALSE}
#another order
data_reorder <- series_3[, c('cpi', 'exchange', 'unemp')]
var3a <- VAR(data_reorder, p = 13, type = "const")
Omega_hat <- summary(var3a)$covres
chol(Omega_hat) %>% t()
```

If we have a structural form, we can obtain impulse response functions and forward error variance decomposition.
With the help of IRFs and FEVDs, we can examine the effect of other variables on the dependent variable (exchange rate), the sign of the impact and its length.

First, we investigate the IRF which means the responsiveness of the exchange rate to shocks on itself, CPI and unemployment rate. Figure 11, 12 and 13 show the IRFs of CPI, unemployment rate and exchange rate respectively.

```{r, eval=FALSE, include=FALSE}
#First try - poor results;
#irf3 <- irf(var3, impulse = "cpi", response = c("exchange"), n.ahead = 20, cumulative = FALSE)
#plot(irf3)
#irf4 <- irf(var3, impulse = "unemp", response = c("exchange"), n.ahead = 20, cumulative = FALSE)
#plot(irf4)
```

```{r, include=FALSE}
irf3a <- irf(var3a, impulse = c('cpi'), response = c("exchange"), n.ahead = 20, cumulative = FALSE)
plot(irf3a)
```

```{r, include=FALSE}
irf4a <- irf(var3a, impulse = "unemp", response = c("exchange"), n.ahead = 20, cumulative = FALSE)
plot(irf4a)
```

```{r, include=FALSE}
irf3c <- irf(var3a, impulse = c('exchange'), response = c("exchange"), n.ahead = 20, cumulative = FALSE)
plot(irf3c)
```

Starting with CPI, the price level increases are supposed to bring negative effect to the USD/CAD exchange rate in the short-run, having its peak in the 4th period. That was expected since the higher prices discourage agents from buying domestic goods, so the demand for the local (Canadian) currency is going down; therefore, the exchange rate USD/CAD decreases (the US dollar becomes more valuable). The impulse response is significant from appr. the second period to the sixth period after which the confidence interval started to have zero within its borders.

The sign of the unemployment rate effect was fluctuating between positive and negative in all periods; however, we can notice that the confidence interval always includes zero, that's why the response is insignificant. 

In case of a structural shock to the exchange rate itself, it gives a positive permanent response for growth of the exchange rate. In the short and medium period the response fluctuates; in long-run, it decreases steadily. 

A shock to the variable will not only have an impact on this variable, but it will also influence other variables. We can see that if we perform forecast error variance decomposition as well. The FEVD allows to analyze the contribution of variable j to the h-step forecast error variance of variable k. We are interested in the exchange rate, so we will only leave the results of FEVD of this variable:

```{r, echo=FALSE}
fevd <- data.frame(fevd(var3a, n.ahead = 12)$exchange)
fevd[, c('exchange', 'cpi','unemp')]
```

As seen, most of the error variance of the exchange rate can be explained by the shocks to their own series. Only up to 3% of the error variance is explained by CPI, while the unemployment rate does not explain anything at all. The same situation can be observed if we take a look at FEVD of CPI or unemployment rate - they can be mostly explained by their own shocks. 

##### Conclusion

To conclude, despite applying more complex methods of econometric analysis, the ARIMA(2,1,4) model turned out to perform better than any suggested specification of the VAR model. While three VAR models that we estimated tended to significantly undervalue the future exchange rate, the ARIMA model suggested that the exchange rate be more or less constant over the following 12 months - though that time it was overvalued. Overall, neither of the models reached high accuracy though the ARIMA model had the lowest value of MSE (and other accuracy metrics) among all other models. Apparently, the VAR model did not perform as well as it could have since one of the variables did not correlate with the exchange rate at all while two other indicators did not explain the error variance of the exchange rate as much. Thus, the exchange rate was mostly explained by its own shocks. Undoubtedly, this research can be enhanced by either applying more modern methods (such as reinforcement learning) or picking up more suitable explanatory variables.

##### References

1. BABU, As & REDDY, SK. "Exchange Rate Forecasting using ARIMA, Neural Network and FuzzyNeuron", *Journal of Stock & Forex Trading*, Vol.4, no.3, 2015.

2. CHEUNG, Yin-Wong & CHINN, Menzie D. & PASCUAL, Antonio Garcia & ZHANG, Yi. "Exchange rate prediction redux: new models, new data, new currencies", *European Central Bank Working Paper Series*, no.2018, 2017.

3. CHEUNG, Yin-Wong & CHINN, Menzie D. & PASCUAL, Antonio Garcia. "Empirical exchange rate models of the nineties: Are any fit to survive?", *Journal of International Money and Finance*, no.24, 2005, p.1150-1175.

4. ITO, Takatoshi & SATO, Kiyotaka. "Exchange Rate Changes and Inflation in Post-Crisis Asian Economies: VAR Analysis of the Exchange Rate Pass-Through", *RIETI Discussion Paper Series*, 2006.

5. MIDA, Jaroslav. "Forecasting Exchange Rates: A VAR Analysis", *Charles University in Prague*, 2013.

6. SIMIONESCU, Mihaela. "Forecast Intervals for US/EURO Foreign Exchange Rate", *Revista de métodos cuantitativos para la economía y la empresa*, no.23, 2017, p.257-271.

##### Attachments

Figure 1:

```{r, echo=FALSE}
autoplot(rate_ts) + ggtitle("Figure 1. Exchange Rate, USD to CAD") + ylab('Exchange Rate')
```

Figure 2:

```{r, echo=FALSE}
plot(decompose(rate_ts))
```

Figure 3:

```{r, echo=FALSE}
grid.arrange(ggAcf(rate_ts), ggPacf(rate_ts), nrow = 2)
```

Figure 4:

```{r, echo=FALSE}
diff_rate <- diff(rate_ts)
autoplot(diff_rate) + ggtitle("Figure 4. Change in Exchange Rate, USD to CAD") + ylab("Exchange rate difference")
```

Figure 5:

```{r, echo=FALSE}
grid.arrange(ggAcf(diff_rate), ggPacf(diff_rate), nrow = 2)
```

Figure 6:

```{r, echo=FALSE}
grid.arrange(ggAcf(diff(train)), ggPacf(diff(train)), nrow = 2)
```

Figure 7:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
actual_data <- window(rate_ts, start = c(2018, 12), end = c(2019,12))
autoplot(fore2, xlim = c(2005,2020)) + autolayer(actual_data) + autolayer(fore2$mean, series = "Forecast") + ylab('Exchange Rate') + ggtitle("Figure 7. Forecast from ARIMA(2, 1, 4)")
```

Figure 8:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
autoplot(fore0, xlim = c(2005,2020)) + autolayer(actual_data, series = "Actual Exchange Rate") + autolayer(fore0$mean, series = "Forecast") + ylab('Exchange Rate') + ggtitle("Figure 8. Forecast from ARIMA(0, 1, 1)")
```

Figure 9:

```{r, echo=FALSE}
autoplot(series) + ggtitle("Figure 9. Series of Four Macroeconomic Indicators") + ylab('Value') 
```

Figutre 10:

```{r, echo=FALSE, message=FALSE, warning=FALSE}
#Forecast VAR(13) w/ cpi and exchange
varfore4 <- forecast(var4, h = 12)
plot7 <- autoplot(varfore4$forecast$exchange, xlim = c(2000,2020),  main = "Figure 10. Exchange Rate Forecast from VAR(13)") + autolayer(actual_data2[,1], series = "Actual Value") + autolayer(varfore4$forecast$exchange$mean, series = "Forecast") + ylab('Exchange Rate')
plot7
```

Figure 11:

```{r, echo=FALSE}
irf3a <- irf(var3a, impulse = c('cpi'), response = c("exchange"), n.ahead = 20, cumulative = FALSE)
plot(irf3a)
```

Figure 12:

```{r, echo=FALSE}
irf4a <- irf(var3a, impulse = "unemp", response = c("exchange"), n.ahead = 20, cumulative = FALSE)
plot(irf4a)
```

Figure 13:

```{r, echo=FALSE}
irf3c <- irf(var3a, impulse = c('exchange'), response = c("exchange"), n.ahead = 20, cumulative = FALSE)
plot(irf3c)
```


